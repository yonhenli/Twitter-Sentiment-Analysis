{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B6A Technical Challenge - Sentiment Analysis #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the Data ##\n",
    "Let's take advantage of pandas to read the csv file and convert it to pandas.DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use read_csv() to read csv file, decode the tweets using 'ISO-8859-1' standard and rename every column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets = pd.read_csv('../training.1600000.processed.noemoticon.csv',\n",
    "        encoding = \"ISO-8859-1\", names=['polarity','tweet_id','date',\n",
    "        'query','user_id','tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>polarity</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>date</th>\n",
       "      <th>query</th>\n",
       "      <th>user_id</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   polarity    tweet_id                          date     query  \\\n",
       "0         0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY   \n",
       "1         0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
       "2         0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
       "3         0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "4         0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "\n",
       "           user_id                                              tweet  \n",
       "0  _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
       "1    scotthamilton  is upset that he can't update his Facebook by ...  \n",
       "2         mattycus  @Kenichan I dived many times for the ball. Man...  \n",
       "3          ElleCTF    my whole body feels itchy and like its on fire   \n",
       "4           Karoli  @nationwideclass no, it's not behaving at all....  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# roughly check the structure of the dataset\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are only interested in polarity and tweet, we take out polarity and tweet and put them into numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_set = tweets['tweet'].values\n",
    "score_set = tweets['polarity'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Pre-processing ##\n",
    "Logistic regression algorithm requires numerical vectors to perform classification tasks. We are going to convert every tweet into a vector consisting of different words. We are going to perform the following precedures to pre-process tweets:\n",
    "\n",
    "1. remove non-letters\n",
    "2. restore contractions\n",
    "3. remove punctuation\n",
    "4. tokenize tweet\n",
    "4. remove stopwords\n",
    "5. stem words\n",
    "\n",
    "We first import some packages for test pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define some constants for text pre-processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ENGLISH_STOPWORDS = set(stopwords.words('english'))\n",
    "CONTRACTIONS = [\n",
    "    (r'\\'m', ' am'),\n",
    "    (r'\\'ll', ' will'),\n",
    "    (r'\\'s', ' s'),\n",
    "    (r'\\'d', ' had'),\n",
    "    (r'aren\\'t', 'are not'),\n",
    "    (r'can\\'t', 'can not'),\n",
    "    (r'couldn\\'t', 'could not'),\n",
    "    (r'daren\\'t', 'dare not'),\n",
    "    (r'didn\\'t', 'did not'),\n",
    "    (r'doesn\\'t', 'does not'),\n",
    "    (r'don\\'t', 'do not'),\n",
    "    (r'isn\\'t', 'is not'),\n",
    "    (r'hasn\\'t', 'has not'),\n",
    "    (r'haven\\'t', 'have not'),\n",
    "    (r'hadn\\'t', 'had not'),\n",
    "    (r'mayn\\'t', 'may not'),\n",
    "    (r'mightn\\'t', 'might not'),\n",
    "    (r'mustn\\'t', 'must not'),\n",
    "    (r'needn\\'t', 'need not'),\n",
    "    (r'oughtn\\'t', 'ought not'),\n",
    "    (r'shan\\'t', 'shall not'),\n",
    "    (r'shouldn\\'t', 'should not'),\n",
    "    (r'wasn\\'t', 'was not'),\n",
    "    (r'weren\\'t', 'were not'),\n",
    "    (r'won\\'t', 'will not'),\n",
    "    (r'wouldn\\'t', 'would not'),\n",
    "    (r'ain\\'t', 'am not')\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a function to convert each tweet to a list of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "this function process split each tweet into a list of words.\n",
    "\n",
    "args:\n",
    "    tweet: a tweet (string)\n",
    "\n",
    "returns:\n",
    "    a list of words (list)\n",
    "\"\"\"\n",
    "def process_tweet(tweet):\n",
    "    # remove non-letters\n",
    "    tweet = re.sub(\"[^a-zA-Z]\",\" \", tweet)\n",
    "\n",
    "    # restore contractions \n",
    "    for pattern in CONTRACTIONS:\n",
    "        tweet = re.sub(pattern[0], pattern[1], tweet)\n",
    "    \n",
    "    # remove punctuation\n",
    "    nopunc = [char for char in tweet if char not in string.punctuation]\n",
    "    nopunc = ''.join(nopunc) # join characters together\n",
    "    \n",
    "    # convert words to lower case and tokenize them\n",
    "    words = nopunc.lower().split()\n",
    "    \n",
    "    # remove stopwords\n",
    "    words = [word for word in words if word not in ENGLISH_STOPWORDS]\n",
    "    \n",
    "    # stemming each word\n",
    "    stemmer = PorterStemmer()\n",
    "    words = [stemmer.stem(word) for word in words]\n",
    "    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Featue Matrix ##\n",
    "In this part, we will convert text to numbers using the Bag-of-Words model, or BoW. This model ignores the order information and focuses on the occurrence of words in a document. This can be done by assigning each word a unique number. Then any document we see can be encoded as a fixed-length vector with the length of the vocabulary of known words. The value in each position in the vector could be filled with a count or frequency of each word in the encoded document. To use BoW, Sciki-Learn provides three different schemes, **CountVectorizer**, **TfidfVectorizer**, **HashingVectorizer**. <br> In order to better leverage our logistic regression algorithm, we will use CountVectorizer for this project. Since there are so many features, we can expect a lot of zero counts for the presence of that word in that document. Because of this, SciKit Learn will output a **Sparse Matrix**.\n",
    "\n",
    "We first import some useful packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import sparse, stats\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define maximum amount of features we want. We will build a vocabulary that only consider the top MAX_FEATURES ordered by **term frequency** across the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_FEATURES = 5000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a function to train the bag-of-word transformer that is used to transform tweets to feature matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "this function trains CountVectorizer model with the trainning set and extract\n",
    "the features we want.\n",
    "\n",
    "args:\n",
    "    tweet_arr: the whole trainning set (numpy.array)\n",
    "returns:\n",
    "    bow_transformer: bag-of-word transformer (scipy.sparse.csr_matrix)\n",
    "\"\"\"\n",
    "def make_bow_transformer(tweet_arr):\n",
    "    # this function fits the countvectorizer\n",
    "    bow_transformer = CountVectorizer(tokenizer=lambda doc: doc, \n",
    "        lowercase=False, max_features=MAX_FEATURES).fit(tweet_arr)\n",
    "\n",
    "    return bow_transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function computes the NxM feature matrix where N is the number of tweets and M is the number of the features.<br> In addition, we add one more feature that is set as 1 for every tweet to compute the **bias** term, **b**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "this function transforms trainning set to feature matrix using \n",
    "bag-of-word model. it computes teh NxM matrix where N is the \n",
    "number of tweets and M is the number of the features.\n",
    "\n",
    "args:\n",
    "    tweet_arr: trainning set (numpy.array)\n",
    "    bow_transformer: bag-of-word transformer (scipy.sparse.csr_matrix)\n",
    "returns:\n",
    "    tweet_bow: feature matrix (scipy.sparse.csr_matrix)\n",
    "\"\"\"\n",
    "def compute_feature_matrix(tweet_arr, bow_transformer):\n",
    "    # vetorize tweets\n",
    "    tweet_bow = bow_transformer.transform(tweet_arr)\n",
    "\n",
    "    # to compute bias, add one more feature that has set to be 1 for every tweet\n",
    "    bias_vector = np.ones(tweet_bow.shape[0])[:,None]\n",
    "    tweet_bow = sparse.hstack((tweet_bow,bias_vector),\n",
    "     dtype=tweet_bow.dtype).toarray()\n",
    "\n",
    "    return tweet_bow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression ##\n",
    "After pre-process the tweets, we can work on out logistic regression module now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid Function ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "sigmoid function\n",
    "\n",
    "args:\n",
    "    a: a matrix (numpy.array)\n",
    "\n",
    "returns:\n",
    "    a matrix (numpy.array)\n",
    "\"\"\"\n",
    "def sigmoid(a):\n",
    "    return 1.0 / (1.0 + np.exp(-1.0 * a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost Function ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the probability function calculate the probablity of the sentiment of each tweet being categorized as 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "function to calculate the probability\n",
    "\n",
    "args:\n",
    "    feature: feature of a single tweet (numpy.array)\n",
    "    weight: weight vector (numpy.array)\n",
    "\n",
    "returns:\n",
    "    a matrix (numpy.array)\n",
    "\"\"\"\n",
    "def prob_function(feature, weight):\n",
    "    # np dot matrix multiplication\n",
    "    return sigmoid(np.dot(feature, weight))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then define the cost function to calculate the cost value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "cost function\n",
    "\n",
    "args:\n",
    "\tw: weight vector (numpy.array)\n",
    "\tx: trainning set (numpy.array)\n",
    "\ty: score set (numpy.array)\n",
    "\t\n",
    "returns:\n",
    "\tthe cost value (float)\n",
    "\"\"\"\n",
    "def cost(w, x, y):\n",
    "    [n,m] = x.shape\n",
    "    cost_value = 0\n",
    "\n",
    "    for i in range(n):\n",
    "        cost_value += (y[i] * np.log(prob_function(x[i], w)) +\n",
    "                       (1.0 - y[i]) * np.log(1.0 - prob_function(x[i], w)))\n",
    "    \n",
    "    # since the sign of cost value in the gradient descent formula\n",
    "    # in negative, so we times a -1.0 to the result\n",
    "    return -1.0 * cost_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first define some constants for gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SPEED = 0.001\n",
    "MAX_ITERATIONS = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a function for stochastic gradient descent. We will randomly shuffle the data set at the beginning of each epoch. For this project, we won't check whether the model converges or not after each epoch. Instead, we will make the model only run for 2 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Gradient descent (stochastic) to find the local minimum of cost function. \n",
    "\n",
    "args:\n",
    "    train_set: the trainning input (numpy.array)\n",
    "    score_set: the trainning output (numpy.array)\n",
    "\n",
    "returns:\n",
    "    the weight vector (numpy.array)\n",
    "\"\"\"\n",
    "def gradient_descent(train_set, score_set):\n",
    "    # get the shape of the train set\n",
    "    [row, column] = train_set.shape\n",
    "    \n",
    "    # initialize weight vector\n",
    "    weight_vector = np.zeros(column)\n",
    "    \n",
    "    # variables to show stats\n",
    "    iteration = 0\n",
    "    last_cost_val = 0\n",
    "\n",
    "    while (iteration < MAX_ITERATIONS):\n",
    "        # shuffle training set at the beginning of each epoch\n",
    "        indices = np.random.permutation(train_set.shape[0])\n",
    "        train_set, score_set = train_set[indices], score_set[indices]\n",
    "        \n",
    "        # stochastic gradient descent\n",
    "        for i in range(row):\n",
    "            prediction = prob_function(train_set[i], weight_vector)\n",
    "            error = score_set[i] - prediction\n",
    "            weight_vector = weight_vector + SPEED * train_set[i].transpose() * error\n",
    "        \n",
    "        cost_val = cost(weight_vector, train_set, score_set)\n",
    "        \n",
    "        # calculate the accuracy of current weight\n",
    "        valid = 0\n",
    "        for i in range(row):\n",
    "            if prob_function(weight_vector, train_set[i]) >= 0.5:\n",
    "                prediction = 1\n",
    "            else:\n",
    "                prediction = 0\n",
    "            \n",
    "            if prediction == score_set[i] :\n",
    "                valid += 1\n",
    "        \n",
    "        percent = 100.0 * valid / row\n",
    "        \n",
    "        iteration += 1\n",
    "        \n",
    "        # show stats\n",
    "        print(\"iteration %d...\" % iteration)\n",
    "        print('cost value: %.4f' % cost_val)\n",
    "        print('well-classified tweets: {0} / {1} ({2}%)'.format(valid, row, percent))\n",
    "        print()\n",
    "\n",
    "    return weight_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Logistic Regression Model ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first define a constant for trainning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NUM_PER_GROUP = 100000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a function to help us seperate trainning set into several groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "this function helps to split an array into several groups.\n",
    "\n",
    "args:\n",
    "\tn:\tthe number of items in each group (int)\n",
    "\titerable: an iterable object (object)\n",
    "\n",
    "returns:\n",
    "\ta tuple consisting tuples with same amoumt of items (tuple) \n",
    "\"\"\"\n",
    "def grouper(n, iterable):\n",
    "    args = [iter(iterable)] * n\n",
    "    return itertools.zip_longest(*args, fillvalue=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a function to train the logistic regression model. \n",
    "\n",
    "In this part, I mainly conquered 2 challenges:\n",
    "1. **the challenge of handling large amount of data set**. If we compute the whole 1.6 million tweets at once, one problem is that we cannot have too many features because the computing process requires huge amount of memory. In order to incorporate more features, we shuffle the training set first and then split the 1.6 million tweets into 16 groups with each group having 10,0000 tweets. Then we compute the weight vectors of these 16 groups seperately. At the end, a final weight vector is generated by calculating the weighted average of the 16 weight vectors.\n",
    "2. **the challenge of setiing the initial weight vector**. Because the instruction requires us to run 2 epochs, we cannot guarantee that the model actually converges by only runing 2 epochs. Thus, choosing the initial weight vector is critical for obtaining a good model. I have tried to fill the intial weight vector with all 0s, 1s and numbers uniformly distributed between -1 and 1. It turned out that having initial vector set as all 0s trains a better model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "this functino trains the model.\n",
    "\n",
    "args:\n",
    "    train_set: the trainning input (numpy.array)\n",
    "    score_set: the trainning output (numpy.array)\n",
    "returns:\n",
    "    weight_vector: trained weight vector (numpy.array)\n",
    "    bow_transformer: bag-of-word transformer (scipy.sparse.csr_matrix)\n",
    "\"\"\"\n",
    "def train(train_set,score_set):\n",
    "    # tokenize tweets here\n",
    "    tweet_list = []\n",
    "    for tweet in train_set:\n",
    "        tweet_list.append(process_tweet(tweet))\n",
    "\n",
    "    # convert tweet_list to numpy array\n",
    "    tweet_arr = np.array(tweet_list)\n",
    "\n",
    "    # train bag-of-words prob_function\n",
    "    bow_transformer = make_bow_transformer(tweet_arr)\n",
    "\n",
    "    # to store weight vectors for each group\n",
    "    weight_vector_list = []\n",
    "\n",
    "    # index to split trainning set into n group\n",
    "    group_index_list = []\n",
    "\n",
    "    # get the number of tweets in trainning set\n",
    "    num_items = len(train_set)\n",
    "\n",
    "    # split trainning data into several groups\n",
    "    group_index_list = list(grouper(NUM_PER_GROUP,\n",
    "     np.random.permutation(num_items)))\n",
    "\n",
    "    print('==== Start training...')\n",
    "\n",
    "    for i in range(len(group_index_list)):\n",
    "        # quit if it is not a complete group\n",
    "        if len(group_index_list[i]) < NUM_PER_GROUP:\n",
    "            break\n",
    "\n",
    "        print(\"group # \", i+1)\n",
    "        \n",
    "        # compute feature matrix\n",
    "        feature_array = compute_feature_matrix(\n",
    "            tweet_arr[list(group_index_list[i])], bow_transformer)\n",
    "        \n",
    "        # transform score set\n",
    "        score_matrix = score_set[list(group_index_list[i])]\n",
    "        score_array = np.transpose(np.array(score_matrix))\n",
    "        \n",
    "        weight_vector_part = gradient_descent(feature_array, score_array)\n",
    "        weight_vector_list.append(weight_vector_part)\n",
    "    \n",
    "    # compute weighted average of the weight vector of each group\n",
    "    weight_vector = np.mean(weight_vector_list, axis=0)\n",
    "    \n",
    "    print('==== Done')\n",
    "    \n",
    "    return weight_vector, bow_transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Invoke Training ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== training model...\n",
      "==== Start training...\n",
      "group #  1\n",
      "iteration 1...\n",
      "cost value: 60908.4799\n",
      "well-classified tweets: 71860 / 100000 (71.86%)\n",
      "\n",
      "iteration 2...\n",
      "cost value: 58164.3824\n",
      "well-classified tweets: 72882 / 100000 (72.882%)\n",
      "\n",
      "group #  2\n",
      "iteration 1...\n",
      "cost value: 60965.8512\n",
      "well-classified tweets: 71005 / 100000 (71.005%)\n",
      "\n",
      "iteration 2...\n",
      "cost value: 58176.4536\n",
      "well-classified tweets: 72726 / 100000 (72.726%)\n",
      "\n",
      "group #  3\n",
      "iteration 1...\n",
      "cost value: 60864.3668\n",
      "well-classified tweets: 71391 / 100000 (71.391%)\n",
      "\n",
      "iteration 2...\n",
      "cost value: 58154.4848\n",
      "well-classified tweets: 72885 / 100000 (72.885%)\n",
      "\n",
      "group #  4\n",
      "iteration 1...\n",
      "cost value: 61042.8048\n",
      "well-classified tweets: 71448 / 100000 (71.448%)\n",
      "\n",
      "iteration 2...\n",
      "cost value: 58311.7815\n",
      "well-classified tweets: 72458 / 100000 (72.458%)\n",
      "\n",
      "group #  5\n",
      "iteration 1...\n",
      "cost value: 60878.2248\n",
      "well-classified tweets: 71610 / 100000 (71.61%)\n",
      "\n",
      "iteration 2...\n",
      "cost value: 58111.9396\n",
      "well-classified tweets: 72728 / 100000 (72.728%)\n",
      "\n",
      "group #  6\n",
      "iteration 1...\n",
      "cost value: 61107.0019\n",
      "well-classified tweets: 71467 / 100000 (71.467%)\n",
      "\n",
      "iteration 2...\n",
      "cost value: 58386.3826\n",
      "well-classified tweets: 72483 / 100000 (72.483%)\n",
      "\n",
      "group #  7\n",
      "iteration 1...\n",
      "cost value: 60935.3885\n",
      "well-classified tweets: 71208 / 100000 (71.208%)\n",
      "\n",
      "iteration 2...\n",
      "cost value: 58220.2626\n",
      "well-classified tweets: 72234 / 100000 (72.234%)\n",
      "\n",
      "group #  8\n",
      "iteration 1...\n",
      "cost value: 60904.1973\n",
      "well-classified tweets: 71696 / 100000 (71.696%)\n",
      "\n",
      "iteration 2...\n",
      "cost value: 58155.1662\n",
      "well-classified tweets: 72612 / 100000 (72.612%)\n",
      "\n",
      "group #  9\n",
      "iteration 1...\n",
      "cost value: 60801.3975\n",
      "well-classified tweets: 71626 / 100000 (71.626%)\n",
      "\n",
      "iteration 2...\n",
      "cost value: 58078.2516\n",
      "well-classified tweets: 72589 / 100000 (72.589%)\n",
      "\n",
      "group #  10\n",
      "iteration 1...\n",
      "cost value: 60881.7564\n",
      "well-classified tweets: 71519 / 100000 (71.519%)\n",
      "\n",
      "iteration 2...\n",
      "cost value: 58140.1985\n",
      "well-classified tweets: 72689 / 100000 (72.689%)\n",
      "\n",
      "group #  11\n",
      "iteration 1...\n",
      "cost value: 61126.5081\n",
      "well-classified tweets: 71162 / 100000 (71.162%)\n",
      "\n",
      "iteration 2...\n",
      "cost value: 58443.2978\n",
      "well-classified tweets: 72072 / 100000 (72.072%)\n",
      "\n",
      "group #  12\n",
      "iteration 1...\n",
      "cost value: 60956.5994\n",
      "well-classified tweets: 71383 / 100000 (71.383%)\n",
      "\n",
      "iteration 2...\n",
      "cost value: 58182.1448\n",
      "well-classified tweets: 72272 / 100000 (72.272%)\n",
      "\n",
      "group #  13\n",
      "iteration 1...\n",
      "cost value: 61084.0101\n",
      "well-classified tweets: 70451 / 100000 (70.451%)\n",
      "\n",
      "iteration 2...\n",
      "cost value: 58390.6455\n",
      "well-classified tweets: 72330 / 100000 (72.33%)\n",
      "\n",
      "group #  14\n",
      "iteration 1...\n",
      "cost value: 60999.2287\n",
      "well-classified tweets: 71691 / 100000 (71.691%)\n",
      "\n",
      "iteration 2...\n",
      "cost value: 58258.6494\n",
      "well-classified tweets: 72675 / 100000 (72.675%)\n",
      "\n",
      "group #  15\n",
      "iteration 1...\n",
      "cost value: 60882.1626\n",
      "well-classified tweets: 71381 / 100000 (71.381%)\n",
      "\n",
      "iteration 2...\n",
      "cost value: 58138.0324\n",
      "well-classified tweets: 72745 / 100000 (72.745%)\n",
      "\n",
      "group #  16\n",
      "iteration 1...\n",
      "cost value: 60983.9233\n",
      "well-classified tweets: 71423 / 100000 (71.423%)\n",
      "\n",
      "iteration 2...\n",
      "cost value: 58214.2105\n",
      "well-classified tweets: 72517 / 100000 (72.517%)\n",
      "\n",
      "==== Done\n",
      "===== model trained!\n"
     ]
    }
   ],
   "source": [
    "# modify positive sentiment score from 4 to 1\n",
    "score_set[score_set == 4] = 1\n",
    "\n",
    "# shuffle dataset\n",
    "indices = np.random.permutation(score_set.shape[0])\n",
    "x_train, y_train = train_set[indices], score_set[indices]\n",
    "\n",
    "# train the model\n",
    "print ('===== training model...')\n",
    "weight_vector, bow_transformer = train(train_set, score_set)\n",
    "print ('===== model trained!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outputs ##\n",
    "After running this jupyter notebook on AWS EC2, we have the following outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bottom 5 words by weight: \n",
      "----> miss\n",
      "----> sad\n",
      "----> wish\n",
      "----> hate\n",
      "----> sorri\n",
      "top 5 words by weight: \n",
      "----> thank\n",
      "----> love\n",
      "----> good\n",
      "----> great\n",
      "----> happi\n",
      "the value of the bias: \n",
      "----> 0.0393152994258\n"
     ]
    }
   ],
   "source": [
    "# sort the weight vector\n",
    "w_output = weight_vector.argsort()\n",
    "\n",
    "print(\"bottom 5 words by weight: \")\n",
    "for i in w_output[:5]:\n",
    "    if (i != len(w_output) - 1):\n",
    "        print(\"---->\", bow_transformer.get_feature_names()[i])\n",
    "\n",
    "print(\"top 5 words by weight: \")\n",
    "for i in w_output[:-6:-1]:\n",
    "    if (i != len(w_output) - 1):\n",
    "        print(\"---->\", bow_transformer.get_feature_names()[i])\n",
    "\n",
    "print(\"the value of the bias: \")\n",
    "print(\"---->\", weight_vector[-1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
